"""
FastAPI service for RAG system.
Handles prompt ingestion, vector retrieval via ChromaDB, LLM inference via Ollama,
and observability via Prometheus metrics.
"""

import os

# To suppress info/warning message from TensorFlow (sentence-transformers)
os.environ["TF_ENABLE_ONEDNN_OPTS"] = "0"

import time
from pathlib import Path

import chromadb
from fastapi import FastAPI
from fastapi.responses import Response
from langchain_huggingface import HuggingFaceEmbeddings
from ollama import Client as OllamaClient
from prometheus_client import Counter, Histogram, Gauge, generate_latest

# Local application imports
from app.schemas import ChatRequest, ChatResponse
from database import save_prompt_response, init_db

# Persistent ChromaDB directory
CHROMA_DB_PATH = Path(__file__).resolve().parent / "chroma_db_data"

# Prometheus metrics
LLM_PROMPTS_TOTAL = Counter(
    "llm_prompts_total", "Total number of prompts sent to the LLM."
)
LLM_LATENCY = Histogram(
    "llm_generation_latency_seconds", "Time spent generating LLM responses."
)
LLM_LATENCY_RAW = Gauge(
    "llm_generation_latency_raw_seconds",
    "LLM generation latency per prompt (raw value).",
)
RAG_RETRIEVED_CHUNKS = Histogram(
    "rag_retrieved_chunks", "Number of chunks retrieved from vector DB."
)
RAG_EMPTY_RESULTS = Counter(
    "rag_empty_results_total", "Number of queries with zero retrieved chunks."
)
LLM_TOKEN_GENERATED = Counter(
    "llm_tokens_generated_total", "Total number of tokens generated by the LLM."
)

# Initialize ChromaDB persistent client
client = chromadb.PersistentClient(path=str(CHROMA_DB_PATH))
# Note: Collection name and distance metric must match ingestion pipeline
collection = client.get_or_create_collection(
    name="papers",
    metadata={"hnsw:space": "cosine"},  # same as ingestion
)

# Embeddings model (PyTorch)
embeddings_model = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2",
    model_kwargs={"device": "cpu"},
)

# Initialize local SQLite DB
init_db()

# Initialize Ollama client
ollama_client = OllamaClient()

# Initialize FastAPI app
app = FastAPI()


# Retrieve relevant document chunks from ChromaDB
def query_chroma(user_query: str, top_k: int = 3, distance_threshold: float = 0.3):
    query_vector = embeddings_model.embed_documents([user_query])[0]
    results = collection.query(query_embeddings=[query_vector], n_results=top_k)

    retrieved_chunks = []
    for doc, dist in zip(results["documents"][0], results["distances"][0]):
        if dist < distance_threshold:
            retrieved_chunks.append(doc)

    return retrieved_chunks


# Combine retrieved chunks and user prompt into final prompt
def build_prompt(user_prompt: str, context_chunks: list):
    if context_chunks:
        context_text = "\n\n".join(context_chunks)
        return f"Use the following context to answer the question:\n{context_text}\n\nQuestion: {user_prompt}"
    else:
        return user_prompt


# Metrics endpoint
@app.get("/metrics")
def metrics():
    return Response(generate_latest(), media_type="text/plain")


# Chat endpoint
@app.post("/chat", response_model=ChatResponse)
def chat(req: ChatRequest):
    try:
        # Count prompts
        LLM_PROMPTS_TOTAL.inc()

        # Retrieve context from ChromaDB
        retrieved = query_chroma(req.prompt)

        # RAG metrics
        RAG_RETRIEVED_CHUNKS.observe(len(retrieved))
        if len(retrieved) == 0:
            RAG_EMPTY_RESULTS.inc()

        # Build final prompt
        final_prompt = build_prompt(req.prompt, retrieved)

        # LLM call | Send to Ollama phi3-mini
        llm_start = time.time()
        response = ollama_client.generate(model="phi3:mini", prompt=final_prompt)
        llm_latency = time.time() - llm_start
        LLM_LATENCY.observe(llm_latency)
        LLM_LATENCY_RAW.set(llm_latency)

        # Token metrics (from Ollama response)
        tokens = response.eval_count
        LLM_TOKEN_GENERATED.inc(tokens)

        # Model response
        model_answer = response.response

        # For full model response object, use:
        # model_answer = response

        # Save to SQLite DB
        save_prompt_response(req.user_id, req.prompt, model_answer)

        return {
            "response": model_answer,
            "retrieved_chunks": retrieved,
        }

    except Exception as e:
        print("Error in /chat:", e)
        return {"error": str(e)}
